{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CARREGA BIBLIOTECAS\n",
    "\n",
    "import nltk, re, pprint, string, csv, gensim\n",
    "#from nltk import word_tokenize, FreqDist\n",
    "#from nltk.corpus import stopwords\n",
    "#from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
    "#from gensim import corpora\n",
    "#from gensim.corpora import Dictionary\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading https://files.pythonhosted.org/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\vbertalan\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from sklearn) (0.21.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\users\\vbertalan\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.3.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\vbertalan\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\users\\vbertalan\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.17.3)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1321 sha256=df7eadc30663c7bf05f7e86fdaca3ff463a6faeed43282fc88ce4d33adac3045\n",
      "  Stored in directory: C:\\Users\\vbertalan\\AppData\\Local\\pip\\Cache\\wheels\\76\\03\\bb\\589d421d27431bcd2c6da284d5f2286c8e3b2ea3cf1594c074\n",
      "Successfully built sklearn\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LIMPA CSV (salvar com encoding ISO 8859-1 no Sublime, depois de remover a formatação no Bloco de Notas)\n",
    "\n",
    "## PALAVRAS REMOVIDAS PONTUALMENTE\n",
    "remover = ['fls', 'nº', 'ii', 'i','é','i','§', 'sp', 'cc', 'iv', '2º', 'fl', 'r', 'n', 'dr', 'mm', 'c', 'iii']\n",
    "\n",
    "## TRANSFORMAÇÃO DO ARQUIVO\n",
    "with open(\"homicidio_rotulado\",'r') as inFile, open(\"homicidio_rotulado_limpo\",'w') as outFile:\n",
    "    for line in inFile.readlines():\n",
    "        print(\" \".join([word for word in line.lower().translate(str.maketrans('', '', string.punctuation)).split() \n",
    "            if word not in stopwords.words('portuguese') and word not in remover]), file=outFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACHA PALAVRAS MAIS COMUNS\n",
    "\n",
    "f=open('homicidio_limpo','r')\n",
    "raw=f.read()\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "text = nltk.Text(tokens)\n",
    "fdist = FreqDist(text)\n",
    "fdist.most_common(250)\n",
    "len(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CARREGA ARQUIVO - SEM STEMMING\n",
    "\n",
    "with open(\"homicidio_limpo\", newline=\"\\n\") as text:\n",
    "    reader = csv.reader(text, delimiter=\" \")\n",
    "    texto = []\n",
    "    for i in reader:\n",
    "        texto.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CARREGA ARQUIVO NO NLTK\n",
    "\n",
    "import string, csv, nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from collections import Counter\n",
    "\n",
    "def listToString(s):  \n",
    "    # initialize an empty string \n",
    "    str1 = \"\"  \n",
    "    # traverse in the string   \n",
    "    for ele in s:  \n",
    "        str1 += ele\n",
    "        str1 += \" \"\n",
    "    # return string   \n",
    "    return str1  \n",
    "\n",
    "with open(\"homicidio_rotulado\", newline=\"\\n\") as text:\n",
    "    reader = csv.reader(text, delimiter=\",\")\n",
    "    texto = []\n",
    "    textopuro = []\n",
    "    for i in reader:\n",
    "        # split into words\n",
    "        tokens = nltk.word_tokenize(str(i))\n",
    "        # convert to lower case\n",
    "        tokens = [w.lower() for w in tokens]\n",
    "        # remove punctuation from each word\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        # remove remaining tokens that are not alphabetic\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        # filter out stop words        \n",
    "        stop_words = set(stopwords.words('portuguese'))\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "        # stemming\n",
    "        #porter = PorterStemmer()\n",
    "        #stemmed = [porter.stem(word) for word in tokens]\n",
    "        texto.append(nltk.Text(words))\n",
    "        #textopuro.append(listToString(words))\n",
    "        textopuro.append(str(words))\n",
    "        #texto.append(nltk.Text(stemmed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "591\n",
      "178\n",
      "(443, 16957)\n",
      "(148, 16957)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vbertalan\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\vbertalan\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9324324324324325\n",
      "Accuracy: 0.9324324324324325\n",
      "F1 score: 0.9324819324819326\n",
      "Recall: 0.9324324324324325\n",
      "Precision: 0.9328724187879116\n",
      "\n",
      " clasification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93        69\n",
      "           1       0.95      0.92      0.94        79\n",
      "\n",
      "    accuracy                           0.93       148\n",
      "   macro avg       0.93      0.93      0.93       148\n",
      "weighted avg       0.93      0.93      0.93       148\n",
      "\n",
      "\n",
      " confussion matrix:\n",
      " [[65  4]\n",
      " [ 6 73]]\n"
     ]
    }
   ],
   "source": [
    "## USANDO BOW\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "##vectorizer = CountVectorizer(min_df=0, lowercase=False)\n",
    "\n",
    "## especificando bigramas\n",
    "vectorizer = CountVectorizer(min_df=0, lowercase=False, ngram_range=(2,2))\n",
    "\n",
    "vectorizer.fit(textopuro)\n",
    "vector = vectorizer.transform(textopuro).toarray()\n",
    "\n",
    "print(len(vector))\n",
    "print(np.sum(vector[1]))\n",
    "\n",
    "y_values = []\n",
    "\n",
    "with open(\"desfechos.csv\", newline=\"\\n\") as csvfile:\n",
    "    reader = csv.reader(csvfile) # change contents to floats\n",
    "    for i in reader: # each row is a list\n",
    "        #print(i)\n",
    "        y_values.append(i)\n",
    "\n",
    "## retirar caractere estranho na primeira linha\n",
    "y_values = y_values[1:len(y_values)]      \n",
    "\n",
    "## criar conjunto de treinamento e teste\n",
    "##sentences_train, sentences_test, y_train, y_test = train_test_split(vector, y_values, test_size=0.25, random_state=1000)\n",
    "sentences_train, sentences_test, y_train, y_test = train_test_split(textopuro, y_values, test_size=0.25, random_state=1000)\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(sentences_train)\n",
    "X_train = vectorizer.transform(sentences_train)\n",
    "print(X_train.shape)\n",
    "X_test  = vectorizer.transform(sentences_test)\n",
    "print(X_test.shape)\n",
    "\n",
    "## regressão logística\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "print(\"Accuracy:\", score)\n",
    "\n",
    "auto_weighted_prediction = classifier.predict(X_test)\n",
    "print ('Accuracy:', accuracy_score(y_test, auto_weighted_prediction))\n",
    "print ('F1 score:', f1_score(y_test, auto_weighted_prediction,\n",
    "                            average='weighted'))\n",
    "\n",
    "print ('Recall:', recall_score(y_test, auto_weighted_prediction,\n",
    "                              average='weighted'))\n",
    "\n",
    "print ('Precision:', precision_score(y_test, auto_weighted_prediction,\n",
    "                                    average='weighted'))\n",
    "\n",
    "print ('\\n clasification report:\\n', classification_report(y_test,auto_weighted_prediction))\n",
    "\n",
    "print ('\\n confussion matrix:\\n',confusion_matrix(y_test, auto_weighted_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## USANDO TFIDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "##tfidf_vectorizer=TfidfVectorizer(use_idf=True)\n",
    "\n",
    "##especificando bigramas - ngram_range(começo,fim)\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=3,max_features=None,analyzer='word',\n",
    "                             token_pattern=r'\\w{1,}',ngram_range=(2,2),\n",
    "                             use_idf=1,smooth_idf=1)\n",
    "\n",
    "tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(textopuro)\n",
    "print(tfidf_vectorizer_vectors.shape)\n",
    "\n",
    "# get the first vector out (for the first document)\n",
    "first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0]\n",
    " \n",
    "# place tf-idf values in a pandas data frame\n",
    "df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"])\n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)\n",
    "\n",
    "y_values = []\n",
    "\n",
    "with open(\"desfechos.csv\", newline=\"\\n\") as csvfile:\n",
    "    reader = csv.reader(csvfile) # change contents to floats\n",
    "    for i in reader: # each row is a list\n",
    "        #print(i)\n",
    "        y_values.append(i)\n",
    "\n",
    "## retirar caractere estranho na primeira linha\n",
    "y_values = y_values[1:len(y_values)]    \n",
    "\n",
    "## criar conjunto de treinamento e teste\n",
    "##sentences_train, sentences_test, y_train, y_test = train_test_split(vector, y_values, test_size=0.25, random_state=1000)\n",
    "sentences_train, sentences_test, y_train, y_test = train_test_split(textopuro, y_values, test_size=0.25, random_state=1000)\n",
    "vectorizer = TfidfVectorizer(use_idf=True)\n",
    "vectorizer.fit(sentences_train)\n",
    "X_train = vectorizer.transform(sentences_train)\n",
    "print(X_train.shape)\n",
    "X_test  = vectorizer.transform(sentences_test)\n",
    "print(X_test.shape)\n",
    "\n",
    "## regressão logística\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "print(\"Accuracy:\", score)\n",
    "auto_weighted_prediction = classifier.predict(X_test)\n",
    "print ('Accuracy:', accuracy_score(y_test, auto_weighted_prediction))\n",
    "print ('F1 score:', f1_score(y_test, auto_weighted_prediction,\n",
    "                            average='weighted'))\n",
    "\n",
    "print ('Recall:', recall_score(y_test, auto_weighted_prediction,\n",
    "                              average='weighted'))\n",
    "\n",
    "print ('Precision:', precision_score(y_test, auto_weighted_prediction,\n",
    "                                    average='weighted'))\n",
    "\n",
    "print ('\\n clasification report:\\n', classification_report(y_test,auto_weighted_prediction))\n",
    "\n",
    "print ('\\n confussion matrix:\\n',confusion_matrix(y_test, auto_weighted_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "#model = gensim.models.KeyedVectors.load_word2vec_format('skip_s600.txt', binary=False)\n",
    "\n",
    "model = Word2Vec.load(\"embeddings_word2vec_skip_s600.zip\")\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format(‘model.txt’)\n",
    "\n",
    "#wv_from_text = gensim.models.KeyedVectors.load_word2vec_format(datapath('skip_s600.txt'), binary=False)  # C text format\n",
    "#wv_from_bin = gensim.models.KeyedVectors.load_word2vec_format(datapath(\"skip_s600.txt\"), binary=True)  # C bin format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "array is too big; `arr.size * arr.dtype.itemsize` is larger than the maximum possible size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-aa73a53dfcb3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'skip_s600.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m##model = KeyedVectors.load_word2vec_format('skip_s1000.txt')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m   1496\u001b[0m         return _load_word2vec_format(\n\u001b[0;32m   1497\u001b[0m             \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1498\u001b[1;33m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[0;32m   1499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m         \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvector_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m         \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0madd_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: array is too big; `arr.size * arr.dtype.itemsize` is larger than the maximum possible size."
     ]
    }
   ],
   "source": [
    "## IMPORTANDO PELO MÉTODO DO ICMC\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('skip_s600.txt')\n",
    "##model = KeyedVectors.load_word2vec_format('skip_s1000.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-142-f0ab4a491386>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m## retirar caractere estranho na primeira linha\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mx_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mx_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx_values\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m## carrega desfechos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-142-f0ab4a491386>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m## retirar caractere estranho na primeira linha\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mx_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mx_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx_values\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m## carrega desfechos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'list'"
     ]
    }
   ],
   "source": [
    "## USANDO POS\n",
    "\n",
    "import csv\n",
    "import numpy\n",
    "\n",
    "## carrega atributos\n",
    "with open(\"homicidios - atributos.csv\", newline=\"\\n\") as csvfile:\n",
    "    x_values = []\n",
    "    reader = csv.reader(csvfile, delimiter=';') # change contents to floats\n",
    "    for i in reader: # each row is a list\n",
    "        #print(i)\n",
    "        x_values.append(i)\n",
    "\n",
    "## retirar caractere estranho na primeira linha\n",
    "x_values = x_values[1:len(x_values)]   \n",
    "x_values = [int(i) for i in x_values] \n",
    "\n",
    "## carrega desfechos\n",
    "with open(\"desfechos.csv\", newline=\"\\n\") as csvfile:\n",
    "    y_values = []\n",
    "    reader = csv.reader(csvfile) # change contents to floats\n",
    "    for i in reader: # each row is a list\n",
    "        #print(i)\n",
    "        y_values.append(i)\n",
    "\n",
    "## retirar caractere estranho na primeira linha\n",
    "y_values = y_values[1:len(y_values)]  \n",
    "y_values = [int(i) for i in y_values] \n",
    "\n",
    "## criar conjunto de treinamento e teste\n",
    "##sentences_train, sentences_test, y_train, y_test = train_test_split(vector, y_values, test_size=0.25, random_state=1000)\n",
    "sentences_train, sentences_test, y_train, y_test = train_test_split(x_values, y_values, test_size=0.25, random_state=1000)\n",
    "\n",
    "## regressão logística\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(sentences_train, y_train)\n",
    "score = classifier.score(sentences_test, y_test)\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OPERAÇÕES COM NLTK\n",
    "\n",
    "#print(words[:100])\n",
    "#print(texto[0])\n",
    "#texto[0].similar('estado')\n",
    "#texto[0].concordance('estado')\n",
    "\n",
    "#count tokens\n",
    "print(len(texto[0]))\n",
    "#count exclusive words\n",
    "fd = nltk.FreqDist(words)\n",
    "print(len(fd))\n",
    "#most common word\n",
    "fd.max()\n",
    "\n",
    "#tagging in python\n",
    "#tags = nltk.pos_tag(texto[0])\n",
    "#print(\"Parts of speech: \", tags)\n",
    "#counts = Counter( tag for word,  tag in tags)\n",
    "#print(counts)\n",
    "#for el, cnt in counts.items():\n",
    "#    print(el, '{0:2.2f}%'.format((100.0* cnt)/sbase))\n",
    "\n",
    "tags = []\n",
    "\n",
    "for i in range(len(texto)):\n",
    "    tags = nltk.pos_tag(texto[i])\n",
    "    counts = Counter( tag for word,  tag in tags)\n",
    "    tags.append(counts)\n",
    "    print(counts)\n",
    "    \n",
    "#print(type(counts))\n",
    "#print(type(tags))\n",
    "#print(type(Counter(tags[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MAPEANDO AS TAGS PARA UM CSV\n",
    "\n",
    "tags = ['CC','CD','DT','EX','FW','IN','JJ','JJR','JJS','LS','MD','NN','NNS','NNP','NNPS',\n",
    "        'PDT','POS','PRP','PRP$','RB','RBR','RBS','RP','TO','UH','VB','VBD','VBG','VBN',\n",
    "        'VBP','VBZ','WDT','WP','WP$','WRB']\n",
    "\n",
    "def listToString(s):  \n",
    "    # initialize an empty string \n",
    "    str1 = \"\"  \n",
    "    # traverse in the string   \n",
    "    for ele in s:  \n",
    "        str1 += ele   \n",
    "    # return string   \n",
    "    return str1  \n",
    "    \n",
    "def findCount (substring, fullstring):\n",
    "    if substring in fullstring:\n",
    "        pos = fullstring.find(substring)\n",
    "        #print(fullstring[pos+4:pos+9])\n",
    "        #print(onlyNumerics(fullstring[pos+4:pos+9]))        \n",
    "        return (onlyNumerics(fullstring[pos+4:pos+9]))\n",
    "    else:\n",
    "        #print (0)\n",
    "        return (0)\n",
    "\n",
    "def onlyNumerics(seq):\n",
    "    seq_type= type(seq)\n",
    "    return seq_type().join(filter(seq_type.isdigit, seq))\n",
    "\n",
    "with open('pos-tags.csv', mode='w') as outfile:\n",
    "    writer = csv.writer(outfile, delimiter=',')\n",
    "    with open(\"tags\", newline=\"\\n\") as text:\n",
    "        reader = csv.reader(text, delimiter=\" \")\n",
    "        listOfTags = []\n",
    "        for i in reader:\n",
    "            sentence = listToString(i)\n",
    "            for y in tags:\n",
    "                listOfTags.append(findCount(y,sentence))\n",
    "            writer.writerow(listOfTags)\n",
    "            listOfTags = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTANDO SPACY\n",
    "\n",
    "import spacy, csv\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "## conda install -c conda-forge spacy-model-pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OPERAÇÕES COM SPACY\n",
    "\n",
    "doc = nlp(\"Tribunal de Justiça de São Paulo\")\n",
    "print(type(doc))\n",
    "\n",
    "#for token in doc:\n",
    "#    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "#            token.shape_, token.is_alpha, token.is_stop)\n",
    "    \n",
    "for token in doc:\n",
    "    print (token.text, token.pos_, token.pos)\n",
    "\n",
    "''' \n",
    "with open(\"homicidio_limpo\", newline=\"\\n\") as text:\n",
    "    reader = csv.reader(text, delimiter=\" \")\n",
    "    texto = []\n",
    "    for i in reader:\n",
    "        texto.append(i)\n",
    "        \n",
    "print(type(texto[0]))\n",
    "'''\n",
    "\n",
    "'''\n",
    "## via nltk\n",
    "with open(\"homicidio_limpo\", newline=\"\\n\") as text:\n",
    "    reader = csv.reader(text, delimiter=\" \")\n",
    "    texto = []\n",
    "    for i in reader:\n",
    "        # split into words\n",
    "        tokens = nltk.word_tokenize(str(i))\n",
    "        # convert to lower case\n",
    "        tokens = [w.lower() for w in tokens]\n",
    "        # remove punctuation from each word\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        # remove remaining tokens that are not alphabetic\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        # filter out stop words        \n",
    "        stop_words = set(stopwords.words('portuguese'))\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "        # stemming\n",
    "        #porter = PorterStemmer()\n",
    "        #stemmed = [porter.stem(word) for word in tokens]\n",
    "        texto.append(nltk.Text(words))\n",
    "        #texto.append(nltk.Text(stemmed))\n",
    "'''  \n",
    "\n",
    "'''\n",
    "## via spacy\n",
    "with open(\"corrupcao_limpo_sample\", newline=\"\\n\") as text:\n",
    "    reader = csv.reader(text, delimiter=\" \")\n",
    "    texto = []\n",
    "    for i in reader:\n",
    "        nlp = spacy.load('pt_core_news_sm')\n",
    "        # split into words\n",
    "        sentence = nlp(str(i))\n",
    "        tokens = [token.text for token in sentence]\n",
    "        # convert to lower case\n",
    "        tokens = [w.lower() for w in tokens]\n",
    "        # remove punctuation from each word\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        # remove remaining tokens that are not alphabetic\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        # filter out stop words        \n",
    "        stop_words = set(stopwords.words('portuguese'))\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "        # stemming\n",
    "        #porter = PorterStemmer()\n",
    "        #stemmed = [porter.stem(word) for word in tokens]\n",
    "        texto.append(nlp(str((words))))\n",
    "        #texto.append(nltk.Text(stemmed))\n",
    "'''\n",
    "\n",
    "## via spacy, using NLTK, with just a sample of the text\n",
    "with open(\"corrupcao_limpo_sample\", newline=\"\\n\") as text:\n",
    "    reader = csv.reader(text, delimiter=\" \")\n",
    "    texto = []\n",
    "    for i in reader:\n",
    "        # split into words\n",
    "        tokens = nltk.word_tokenize(str(i))\n",
    "        # convert to lower case\n",
    "        tokens = [w.lower() for w in tokens]\n",
    "        # remove punctuation from each word\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        # remove remaining tokens that are not alphabetic\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        # filter out stop words        \n",
    "        stop_words = set(stopwords.words('portuguese'))\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "        # stemming\n",
    "        #porter = PorterStemmer()\n",
    "        #stemmed = [porter.stem(word) for word in tokens]\n",
    "        texto.append(nlp(str(words)))\n",
    "        #texto.append(nltk.Text(stemmed))\n",
    "            \n",
    "print(type(texto))\n",
    "print(type(texto[0]))\n",
    "print(len(texto))\n",
    "print(len(texto[0]))\n",
    "\n",
    "print(texto)\n",
    "\n",
    "#print Counter (token.pos_ for token in texto[0])\n",
    "c = Counter(([token.pos_ for token in texto[0]]))\n",
    "print (c)\n",
    "sbase = sum(c.values())\n",
    "for el, cnt in c.items():\n",
    "    print(el, '{0:2.2f}%'.format((100.0* cnt)/sbase))\n",
    "    \n",
    "#for token in nlp(str(texto[0])):\n",
    "#    print (token.text, token.pos_, token.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(texto[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CARREGA ARQUIVO - COM STEMMING\n",
    "\n",
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "\n",
    "with open(\"homicidio_limpo\", newline=\"\\n\") as texto:\n",
    "    reader = csv.reader(texto, delimiter=\" \")\n",
    "    texto = []\n",
    "    line = []\n",
    "        \n",
    "    for i in reader:\n",
    "        for j in i:\n",
    "            word = stemmer.stem(j)\n",
    "            line.append(word)\n",
    "        texto.append(line)\n",
    "        line = []\n",
    "\n",
    "print(texto[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXEMPLO DE STEMMING EM BLOCO ÚNICO\n",
    "\n",
    "reader = ['provas', 'produzidas', 'acusação', 'restringemse', 'apenas', 'palavras', 'policiais', 'militares', 'responsáveis']\n",
    "print(type(reader))\n",
    "\n",
    "texto = []\n",
    "for i in reader:\n",
    "    word = stemmer.stem(str(i))\n",
    "    texto.append(word)\n",
    "\n",
    "print(len(texto))\n",
    "print(texto)\n",
    "\n",
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "stemmer.stem(\"criminalmente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRIA O DICIONÁRIO DE TERMOS DO CORPUS, EM QUE CADA TERMO ÚNICO GANHA UM INDEX \n",
    "dicionario = corpora.Dictionary(texto)\n",
    "\n",
    "# CONVERTE O DICIONÁRIO EM UMA MATRIZ DE TERMOS \n",
    "doc_matrix_termos = [dicionario.doc2bow(doc) for doc in texto]\n",
    "\n",
    "print(len(doc_matrix_termos))\n",
    "print(len(dicionario))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRIA O LSI USANDO A BIBLIOTECA DO GENSIM\n",
    "\n",
    "lsimodel = LsiModel(corpus=doc_matrix_termos, num_topics=10, id2word=dicionario)\n",
    "lsimodel.show_topics(num_topics=5)  # Showing only the top 5 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsitopics = lsimodel.show_topics(formatted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELAGEM DE TÓPICOS VIA HDP\n",
    "\n",
    "hdpmodel = HdpModel(corpus=doc_matrix_termos, id2word=dicionario)\n",
    "hdpmodel.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdptopics = hdpmodel.show_topics(formatted=False)\n",
    "hdpmodel.save(\"hdp_homicidio_com_stem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRIA O LDA USANDO A BIBLIOTECA DO GENSIM\n",
    "lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# RODA E TREINA O LDA NO DICIONARIO DE TERMOS E NA MATRIZ\n",
    "ldamodel = lda(doc_matrix_termos, num_topics=10, id2word = dicionario, passes=50)\n",
    "ldamodel.save(\"lda_homicidio_com_stem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARREGA MODELO EXISTENTE DE LDA\n",
    "\n",
    "#lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# RODA E TREINA O LDA NO DICIONARIO DE TERMOS E NA MATRIZ\n",
    "#lda.load(\"lda_esquerda\")\n",
    "\n",
    "ldamodel = gensim.models.LdaModel.load('lda_corrupcao1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MOSTRA TÓPICOS DO LDA\n",
    "\n",
    "ldamodel.show_topics(num_topics=10, num_words=20, log=False, formatted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPRIME O RESULTADO DO LDA COM NUM_TOPICS DE TÓPICOS\n",
    "\n",
    "print(ldamodel.print_topics(num_topics=10, num_words=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldatopics = ldamodel.show_topics(formatted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOTA O MODELO DO LDA\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "pyLDAvis.gensim.prepare(ldamodel, doc_matrix_termos, dicionario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEFINE NÚMERO ÓTIMO DE TÓPICOS DO LDA\n",
    "\n",
    "def evaluate_graph(dictionary, corpus, texts, limit):\n",
    "    \"\"\"\n",
    "    Function to display num_topics - LDA graph using c_v coherence\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    limit : topic limit\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    lm_list : List of LDA topic models\n",
    "    c_v : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    c_v = []\n",
    "    lm_list = []\n",
    "    for num_topics in range(1, limit):\n",
    "        lm = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
    "        lm_list.append(lm)\n",
    "        cm = CoherenceModel(model=lm, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        c_v.append(cm.get_coherence())\n",
    "        \n",
    "    # Show graph\n",
    "    x = range(1, limit)\n",
    "    plt.plot(x, c_v)\n",
    "    plt.xlabel(\"num_topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"c_v\"), loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "    return lm_list, c_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RODA O ALGORITMO ACIMA\n",
    "\n",
    "lmlist, c_v = evaluate_graph(dictionary=dicionario, corpus=doc_matrix_termos, texts=texto, limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmtopics = lmlist[5].show_topics(formatted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOTA A AVALIAÇÃO DE TÓPICOS DO LDA\n",
    "\n",
    "pyLDAvis.gensim.prepare(lmlist[2], doc_matrix_termos, dicionario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ENCONTRA OS MELHORES TÓPICOS DO LDA\n",
    "\n",
    "def ret_top_model():\n",
    "    \"\"\"\n",
    "    Since LDAmodel is a probabilistic model, it comes up different topics each time we run it. To control the\n",
    "    quality of the topic model we produce, we can see what the interpretability of the best topic is and keep\n",
    "    evaluating the topic model until this threshold is crossed. \n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    lm: Final evaluated topic model\n",
    "    top_topics: ranked topics in decreasing order. List of tuples\n",
    "    \"\"\"\n",
    "    top_topics = [(0, 0)]\n",
    "    while top_topics[0][1] < 0.97:\n",
    "        lm = LdaModel(corpus=doc_matrix_termos, id2word=dicionario)\n",
    "        coherence_values = {}\n",
    "        for n, topic in lm.show_topics(num_topics=-1, formatted=False):\n",
    "            topic = [word for word, _ in topic]\n",
    "            cm = CoherenceModel(topics=[topic], texts=texto, dictionary=dicionario, window_size=10)\n",
    "            coherence_values[n] = cm.get_coherence()\n",
    "        top_topics = sorted(coherence_values.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return lm, top_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RODA O ALGORITMO ACIMA\n",
    "\n",
    "lm, top_topics = ret_top_model()\n",
    "print(top_topics[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AVALIA TODOS OS MÉTODOS ESTATÍSTICOS \n",
    "\n",
    "lsitopics = [[word for word, prob in topic] for topicid, topic in lsitopics]\n",
    "\n",
    "hdptopics = [[word for word, prob in topic] for topicid, topic in hdptopics]\n",
    "\n",
    "ldatopics = [[word for word, prob in topic] for topicid, topic in ldatopics]\n",
    "\n",
    "lmtopics = [[word for word, prob in topic] for topicid, topic in lmtopics]\n",
    "\n",
    "lsi_coherence = CoherenceModel(topics=lsitopics[:10], texts=texto, dictionary=dicionario, window_size=10).get_coherence()\n",
    "\n",
    "hdp_coherence = CoherenceModel(topics=hdptopics[:10], texts=texto, dictionary=dicionario, window_size=10).get_coherence()\n",
    "\n",
    "lda_coherence = CoherenceModel(topics=ldatopics, texts=texto, dictionary=dicionario, window_size=10).get_coherence()\n",
    "\n",
    "lm_coherence = CoherenceModel(topics=lmtopics, texts=texto, dictionary=dicionario, window_size=10).get_coherence()\n",
    "\n",
    "#lda_lsi_coherence = CoherenceModel(topics=lda_lsi_topics[:10], texts=texto, dictionary=dicionario, window_size=10).get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOTA O GRÁFICO DE AVALIAÇÃO\n",
    "\n",
    "def evaluate_bar_graph(coherences, indices):\n",
    "    \"\"\"\n",
    "    Function to plot bar graph.\n",
    "    \n",
    "    coherences: list of coherence values\n",
    "    indices: Indices to be used to mark bars. Length of this and coherences should be equal.\n",
    "    \"\"\"\n",
    "    assert len(coherences) == len(indices)\n",
    "    n = len(coherences)\n",
    "    x = np.arange(n)\n",
    "    plt.bar(x, coherences, width=0.2, tick_label=indices, align='center')\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Coherence Value')\n",
    "    \n",
    "    \n",
    "#evaluate_bar_graph([lsi_coherence, hdp_coherence, lda_coherence, lm_coherence, lda_lsi_coherence],\n",
    "#                   ['LSI', 'HDP', 'LDA', 'LDA_Mod', 'LDA_LSI'])\n",
    "\n",
    "evaluate_bar_graph([lsi_coherence, hdp_coherence, lda_coherence, lm_coherence],\n",
    "                   ['LSI', 'HDP', 'LDA', 'LDA_Mod'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
