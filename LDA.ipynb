{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CARREGA BIBLIOTECAS\n",
    "\n",
    "import nltk, re, pprint, string, csv, gensim\n",
    "#from nltk import word_tokenize, FreqDist\n",
    "#from nltk.corpus import stopwords\n",
    "#from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
    "#from gensim import corpora\n",
    "#from gensim.corpora import Dictionary\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LIMPA CSV (salvar com encoding ISO 8859-1 no Sublime, depois de remover a formatação no Bloco de Notas)\n",
    "\n",
    "## PALAVRAS REMOVIDAS PONTUALMENTE\n",
    "remover = ['fls', 'nº', 'ii', 'i','é','i','§', 'sp', 'cc', 'iv', '2º', 'fl', 'r', 'n', 'dr', 'mm', 'c', 'iii']\n",
    "\n",
    "## TRANSFORMAÇÃO DO ARQUIVO\n",
    "with open(\"homicidio_rotulado\",'r') as inFile, open(\"homicidio_rotulado_limpo\",'w') as outFile:\n",
    "    for line in inFile.readlines():\n",
    "        print(\" \".join([word for word in line.lower().translate(str.maketrans('', '', string.punctuation)).split() \n",
    "            if word not in stopwords.words('portuguese') and word not in remover]), file=outFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACHA PALAVRAS MAIS COMUNS\n",
    "\n",
    "f=open('homicidio_limpo','r')\n",
    "raw=f.read()\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "text = nltk.Text(tokens)\n",
    "fdist = FreqDist(text)\n",
    "fdist.most_common(250)\n",
    "len(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CARREGA ARQUIVO - SEM STEMMING\n",
    "\n",
    "with open(\"homicidio_limpo\", newline=\"\\n\") as text:\n",
    "    reader = csv.reader(text, delimiter=\" \")\n",
    "    texto = []\n",
    "    for i in reader:\n",
    "        texto.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CARREGA ARQUIVO NO NLTK\n",
    "\n",
    "import string, csv, nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from collections import Counter\n",
    "\n",
    "def listToString(s):  \n",
    "    # initialize an empty string \n",
    "    str1 = \"\"  \n",
    "    # traverse in the string   \n",
    "    for ele in s:  \n",
    "        str1 += ele\n",
    "        str1 += \" \"\n",
    "    # return string   \n",
    "    return str1  \n",
    "\n",
    "with open(\"homicidio_rotulado\", newline=\"\\n\") as text:\n",
    "    reader = csv.reader(text, delimiter=\",\")\n",
    "    texto = []\n",
    "    textopuro = []\n",
    "    for i in reader:\n",
    "        # split into words\n",
    "        tokens = nltk.word_tokenize(str(i))\n",
    "        # convert to lower case\n",
    "        tokens = [w.lower() for w in tokens]\n",
    "        # remove punctuation from each word\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        # remove remaining tokens that are not alphabetic\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        # filter out stop words        \n",
    "        stop_words = set(stopwords.words('portuguese'))\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "        # stemming\n",
    "        #porter = PorterStemmer()\n",
    "        #stemmed = [porter.stem(word) for word in tokens]\n",
    "        texto.append(nltk.Text(words))\n",
    "        #textopuro.append(listToString(words))\n",
    "        textopuro.append(str(words))\n",
    "        #texto.append(nltk.Text(stemmed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## USANDO BOW\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "##vectorizer = CountVectorizer(min_df=0, lowercase=False)\n",
    "\n",
    "## especificando bigramas\n",
    "vectorizer = CountVectorizer(min_df=0, lowercase=False, ngram_range=(2,2))\n",
    "\n",
    "vectorizer.fit(textopuro)\n",
    "vector = vectorizer.transform(textopuro).toarray()\n",
    "\n",
    "print(len(vector))\n",
    "print(np.sum(vector[1]))\n",
    "\n",
    "y_values = []\n",
    "\n",
    "with open(\"desfechos.csv\", newline=\"\\n\") as csvfile:\n",
    "    reader = csv.reader(csvfile) # change contents to floats\n",
    "    for i in reader: # each row is a list\n",
    "        #print(i)\n",
    "        y_values.append(i)\n",
    "\n",
    "## retirar caractere estranho na primeira linha\n",
    "y_values = y_values[1:len(y_values)]      \n",
    "\n",
    "## criar conjunto de treinamento e teste\n",
    "##sentences_train, sentences_test, y_train, y_test = train_test_split(vector, y_values, test_size=0.25, random_state=1000)\n",
    "sentences_train, sentences_test, y_train, y_test = train_test_split(textopuro, y_values, test_size=0.25, random_state=1000)\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(sentences_train)\n",
    "X_train = vectorizer.transform(sentences_train)\n",
    "print(X_train.shape)\n",
    "X_test  = vectorizer.transform(sentences_test)\n",
    "print(X_test.shape)\n",
    "\n",
    "## regressão logística\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "print(\"Accuracy:\", score)\n",
    "\n",
    "auto_weighted_prediction = classifier.predict(X_test)\n",
    "print ('Accuracy:', accuracy_score(y_test, auto_weighted_prediction))\n",
    "print ('F1 score:', f1_score(y_test, auto_weighted_prediction,\n",
    "                            average='weighted'))\n",
    "\n",
    "print ('Recall:', recall_score(y_test, auto_weighted_prediction,\n",
    "                              average='weighted'))\n",
    "\n",
    "print ('Precision:', precision_score(y_test, auto_weighted_prediction,\n",
    "                                    average='weighted'))\n",
    "\n",
    "print ('\\n clasification report:\\n', classification_report(y_test,auto_weighted_prediction))\n",
    "\n",
    "print ('\\n confussion matrix:\\n',confusion_matrix(y_test, auto_weighted_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## USANDO TFIDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "##tfidf_vectorizer=TfidfVectorizer(use_idf=True)\n",
    "\n",
    "##especificando bigramas - ngram_range(começo,fim)\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=3,max_features=None,analyzer='word',\n",
    "                             token_pattern=r'\\w{1,}',ngram_range=(2,2),\n",
    "                             use_idf=1,smooth_idf=1)\n",
    "\n",
    "tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(textopuro)\n",
    "print(tfidf_vectorizer_vectors.shape)\n",
    "\n",
    "# get the first vector out (for the first document)\n",
    "first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0]\n",
    " \n",
    "# place tf-idf values in a pandas data frame\n",
    "df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"])\n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)\n",
    "\n",
    "y_values = []\n",
    "\n",
    "with open(\"desfechos.csv\", newline=\"\\n\") as csvfile:\n",
    "    reader = csv.reader(csvfile) # change contents to floats\n",
    "    for i in reader: # each row is a list\n",
    "        #print(i)\n",
    "        y_values.append(i)\n",
    "\n",
    "## retirar caractere estranho na primeira linha\n",
    "y_values = y_values[1:len(y_values)]    \n",
    "\n",
    "## criar conjunto de treinamento e teste\n",
    "##sentences_train, sentences_test, y_train, y_test = train_test_split(vector, y_values, test_size=0.25, random_state=1000)\n",
    "sentences_train, sentences_test, y_train, y_test = train_test_split(textopuro, y_values, test_size=0.25, random_state=1000)\n",
    "vectorizer = TfidfVectorizer(use_idf=True)\n",
    "vectorizer.fit(sentences_train)\n",
    "X_train = vectorizer.transform(sentences_train)\n",
    "print(X_train.shape)\n",
    "X_test  = vectorizer.transform(sentences_test)\n",
    "print(X_test.shape)\n",
    "\n",
    "## regressão logística\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "print(\"Accuracy:\", score)\n",
    "auto_weighted_prediction = classifier.predict(X_test)\n",
    "print ('Accuracy:', accuracy_score(y_test, auto_weighted_prediction))\n",
    "print ('F1 score:', f1_score(y_test, auto_weighted_prediction,\n",
    "                            average='weighted'))\n",
    "\n",
    "print ('Recall:', recall_score(y_test, auto_weighted_prediction,\n",
    "                              average='weighted'))\n",
    "\n",
    "print ('Precision:', precision_score(y_test, auto_weighted_prediction,\n",
    "                                    average='weighted'))\n",
    "\n",
    "print ('\\n clasification report:\\n', classification_report(y_test,auto_weighted_prediction))\n",
    "\n",
    "print ('\\n confussion matrix:\\n',confusion_matrix(y_test, auto_weighted_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "#model = gensim.models.KeyedVectors.load_word2vec_format('skip_s600.txt', binary=False)\n",
    "\n",
    "model = Word2Vec.load(\"embeddings_word2vec_skip_s600.zip\")\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format(‘model.txt’)\n",
    "\n",
    "#wv_from_text = gensim.models.KeyedVectors.load_word2vec_format(datapath('skip_s600.txt'), binary=False)  # C text format\n",
    "#wv_from_bin = gensim.models.KeyedVectors.load_word2vec_format(datapath(\"skip_s600.txt\"), binary=True)  # C bin format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTANDO PELO MÉTODO DO ICMC\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('skip_s600.txt')\n",
    "##model = KeyedVectors.load_word2vec_format('skip_s1000.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## USANDO POS\n",
    "\n",
    "import csv\n",
    "import numpy\n",
    "\n",
    "## carrega atributos\n",
    "with open(\"homicidios - atributos.csv\", newline=\"\\n\") as csvfile:\n",
    "    x_values = []\n",
    "    reader = csv.reader(csvfile, delimiter=';') # change contents to floats\n",
    "    for i in reader: # each row is a list\n",
    "        #print(i)\n",
    "        x_values.append(i)\n",
    "\n",
    "## retirar caractere estranho na primeira linha\n",
    "x_values = x_values[1:len(x_values)]   \n",
    "x_values = [int(i) for i in x_values] \n",
    "\n",
    "## carrega desfechos\n",
    "with open(\"desfechos.csv\", newline=\"\\n\") as csvfile:\n",
    "    y_values = []\n",
    "    reader = csv.reader(csvfile) # change contents to floats\n",
    "    for i in reader: # each row is a list\n",
    "        #print(i)\n",
    "        y_values.append(i)\n",
    "\n",
    "## retirar caractere estranho na primeira linha\n",
    "y_values = y_values[1:len(y_values)]  \n",
    "y_values = [int(i) for i in y_values] \n",
    "\n",
    "## criar conjunto de treinamento e teste\n",
    "##sentences_train, sentences_test, y_train, y_test = train_test_split(vector, y_values, test_size=0.25, random_state=1000)\n",
    "sentences_train, sentences_test, y_train, y_test = train_test_split(x_values, y_values, test_size=0.25, random_state=1000)\n",
    "\n",
    "## regressão logística\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(sentences_train, y_train)\n",
    "score = classifier.score(sentences_test, y_test)\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OPERAÇÕES COM NLTK\n",
    "\n",
    "#print(words[:100])\n",
    "#print(texto[0])\n",
    "#texto[0].similar('estado')\n",
    "#texto[0].concordance('estado')\n",
    "\n",
    "#count tokens\n",
    "print(len(texto[0]))\n",
    "#count exclusive words\n",
    "fd = nltk.FreqDist(words)\n",
    "print(len(fd))\n",
    "#most common word\n",
    "fd.max()\n",
    "\n",
    "#tagging in python\n",
    "#tags = nltk.pos_tag(texto[0])\n",
    "#print(\"Parts of speech: \", tags)\n",
    "#counts = Counter( tag for word,  tag in tags)\n",
    "#print(counts)\n",
    "#for el, cnt in counts.items():\n",
    "#    print(el, '{0:2.2f}%'.format((100.0* cnt)/sbase))\n",
    "\n",
    "tags = []\n",
    "\n",
    "for i in range(len(texto)):\n",
    "    tags = nltk.pos_tag(texto[i])\n",
    "    counts = Counter( tag for word,  tag in tags)\n",
    "    tags.append(counts)\n",
    "    print(counts)\n",
    "    \n",
    "#print(type(counts))\n",
    "#print(type(tags))\n",
    "#print(type(Counter(tags[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MAPEANDO AS TAGS PARA UM CSV\n",
    "\n",
    "tags = ['CC','CD','DT','EX','FW','IN','JJ','JJR','JJS','LS','MD','NN','NNS','NNP','NNPS',\n",
    "        'PDT','POS','PRP','PRP$','RB','RBR','RBS','RP','TO','UH','VB','VBD','VBG','VBN',\n",
    "        'VBP','VBZ','WDT','WP','WP$','WRB']\n",
    "\n",
    "def listToString(s):  \n",
    "    # initialize an empty string \n",
    "    str1 = \"\"  \n",
    "    # traverse in the string   \n",
    "    for ele in s:  \n",
    "        str1 += ele   \n",
    "    # return string   \n",
    "    return str1  \n",
    "    \n",
    "def findCount (substring, fullstring):\n",
    "    if substring in fullstring:\n",
    "        pos = fullstring.find(substring)\n",
    "        #print(fullstring[pos+4:pos+9])\n",
    "        #print(onlyNumerics(fullstring[pos+4:pos+9]))        \n",
    "        return (onlyNumerics(fullstring[pos+4:pos+9]))\n",
    "    else:\n",
    "        #print (0)\n",
    "        return (0)\n",
    "\n",
    "def onlyNumerics(seq):\n",
    "    seq_type= type(seq)\n",
    "    return seq_type().join(filter(seq_type.isdigit, seq))\n",
    "\n",
    "with open('pos-tags.csv', mode='w') as outfile:\n",
    "    writer = csv.writer(outfile, delimiter=',')\n",
    "    with open(\"tags\", newline=\"\\n\") as text:\n",
    "        reader = csv.reader(text, delimiter=\" \")\n",
    "        listOfTags = []\n",
    "        for i in reader:\n",
    "            sentence = listToString(i)\n",
    "            for y in tags:\n",
    "                listOfTags.append(findCount(y,sentence))\n",
    "            writer.writerow(listOfTags)\n",
    "            listOfTags = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTANDO SPACY\n",
    "\n",
    "import spacy, csv\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "## conda install -c conda-forge spacy-model-pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OPERAÇÕES COM SPACY\n",
    "\n",
    "doc = nlp(\"Tribunal de Justiça de São Paulo\")\n",
    "print(type(doc))\n",
    "\n",
    "#for token in doc:\n",
    "#    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "#            token.shape_, token.is_alpha, token.is_stop)\n",
    "    \n",
    "for token in doc:\n",
    "    print (token.text, token.pos_, token.pos)\n",
    "\n",
    "''' \n",
    "with open(\"homicidio_limpo\", newline=\"\\n\") as text:\n",
    "    reader = csv.reader(text, delimiter=\" \")\n",
    "    texto = []\n",
    "    for i in reader:\n",
    "        texto.append(i)\n",
    "        \n",
    "print(type(texto[0]))\n",
    "'''\n",
    "\n",
    "'''\n",
    "## via nltk\n",
    "with open(\"homicidio_limpo\", newline=\"\\n\") as text:\n",
    "    reader = csv.reader(text, delimiter=\" \")\n",
    "    texto = []\n",
    "    for i in reader:\n",
    "        # split into words\n",
    "        tokens = nltk.word_tokenize(str(i))\n",
    "        # convert to lower case\n",
    "        tokens = [w.lower() for w in tokens]\n",
    "        # remove punctuation from each word\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        # remove remaining tokens that are not alphabetic\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        # filter out stop words        \n",
    "        stop_words = set(stopwords.words('portuguese'))\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "        # stemming\n",
    "        #porter = PorterStemmer()\n",
    "        #stemmed = [porter.stem(word) for word in tokens]\n",
    "        texto.append(nltk.Text(words))\n",
    "        #texto.append(nltk.Text(stemmed))\n",
    "'''  \n",
    "\n",
    "'''\n",
    "## via spacy\n",
    "with open(\"corrupcao_limpo_sample\", newline=\"\\n\") as text:\n",
    "    reader = csv.reader(text, delimiter=\" \")\n",
    "    texto = []\n",
    "    for i in reader:\n",
    "        nlp = spacy.load('pt_core_news_sm')\n",
    "        # split into words\n",
    "        sentence = nlp(str(i))\n",
    "        tokens = [token.text for token in sentence]\n",
    "        # convert to lower case\n",
    "        tokens = [w.lower() for w in tokens]\n",
    "        # remove punctuation from each word\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        # remove remaining tokens that are not alphabetic\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        # filter out stop words        \n",
    "        stop_words = set(stopwords.words('portuguese'))\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "        # stemming\n",
    "        #porter = PorterStemmer()\n",
    "        #stemmed = [porter.stem(word) for word in tokens]\n",
    "        texto.append(nlp(str((words))))\n",
    "        #texto.append(nltk.Text(stemmed))\n",
    "'''\n",
    "\n",
    "## via spacy, using NLTK, with just a sample of the text\n",
    "with open(\"corrupcao_limpo_sample\", newline=\"\\n\") as text:\n",
    "    reader = csv.reader(text, delimiter=\" \")\n",
    "    texto = []\n",
    "    for i in reader:\n",
    "        # split into words\n",
    "        tokens = nltk.word_tokenize(str(i))\n",
    "        # convert to lower case\n",
    "        tokens = [w.lower() for w in tokens]\n",
    "        # remove punctuation from each word\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        # remove remaining tokens that are not alphabetic\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        # filter out stop words        \n",
    "        stop_words = set(stopwords.words('portuguese'))\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "        # stemming\n",
    "        #porter = PorterStemmer()\n",
    "        #stemmed = [porter.stem(word) for word in tokens]\n",
    "        texto.append(nlp(str(words)))\n",
    "        #texto.append(nltk.Text(stemmed))\n",
    "            \n",
    "print(type(texto))\n",
    "print(type(texto[0]))\n",
    "print(len(texto))\n",
    "print(len(texto[0]))\n",
    "\n",
    "print(texto)\n",
    "\n",
    "#print Counter (token.pos_ for token in texto[0])\n",
    "c = Counter(([token.pos_ for token in texto[0]]))\n",
    "print (c)\n",
    "sbase = sum(c.values())\n",
    "for el, cnt in c.items():\n",
    "    print(el, '{0:2.2f}%'.format((100.0* cnt)/sbase))\n",
    "    \n",
    "#for token in nlp(str(texto[0])):\n",
    "#    print (token.text, token.pos_, token.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(texto[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CARREGA ARQUIVO - COM STEMMING\n",
    "\n",
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "\n",
    "with open(\"homicidio_limpo\", newline=\"\\n\") as texto:\n",
    "    reader = csv.reader(texto, delimiter=\" \")\n",
    "    texto = []\n",
    "    line = []\n",
    "        \n",
    "    for i in reader:\n",
    "        for j in i:\n",
    "            word = stemmer.stem(j)\n",
    "            line.append(word)\n",
    "        texto.append(line)\n",
    "        line = []\n",
    "\n",
    "print(texto[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXEMPLO DE STEMMING EM BLOCO ÚNICO\n",
    "\n",
    "reader = ['provas', 'produzidas', 'acusação', 'restringemse', 'apenas', 'palavras', 'policiais', 'militares', 'responsáveis']\n",
    "print(type(reader))\n",
    "\n",
    "texto = []\n",
    "for i in reader:\n",
    "    word = stemmer.stem(str(i))\n",
    "    texto.append(word)\n",
    "\n",
    "print(len(texto))\n",
    "print(texto)\n",
    "\n",
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "stemmer.stem(\"criminalmente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRIA O DICIONÁRIO DE TERMOS DO CORPUS, EM QUE CADA TERMO ÚNICO GANHA UM INDEX \n",
    "dicionario = corpora.Dictionary(texto)\n",
    "\n",
    "# CONVERTE O DICIONÁRIO EM UMA MATRIZ DE TERMOS \n",
    "doc_matrix_termos = [dicionario.doc2bow(doc) for doc in texto]\n",
    "\n",
    "print(len(doc_matrix_termos))\n",
    "print(len(dicionario))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRIA O LSI USANDO A BIBLIOTECA DO GENSIM\n",
    "\n",
    "lsimodel = LsiModel(corpus=doc_matrix_termos, num_topics=10, id2word=dicionario)\n",
    "lsimodel.show_topics(num_topics=5)  # Showing only the top 5 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsitopics = lsimodel.show_topics(formatted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELAGEM DE TÓPICOS VIA HDP\n",
    "\n",
    "hdpmodel = HdpModel(corpus=doc_matrix_termos, id2word=dicionario)\n",
    "hdpmodel.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdptopics = hdpmodel.show_topics(formatted=False)\n",
    "hdpmodel.save(\"hdp_homicidio_com_stem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRIA O LDA USANDO A BIBLIOTECA DO GENSIM\n",
    "lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# RODA E TREINA O LDA NO DICIONARIO DE TERMOS E NA MATRIZ\n",
    "ldamodel = lda(doc_matrix_termos, num_topics=10, id2word = dicionario, passes=50)\n",
    "ldamodel.save(\"lda_homicidio_com_stem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARREGA MODELO EXISTENTE DE LDA\n",
    "\n",
    "#lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# RODA E TREINA O LDA NO DICIONARIO DE TERMOS E NA MATRIZ\n",
    "#lda.load(\"lda_esquerda\")\n",
    "\n",
    "ldamodel = gensim.models.LdaModel.load('lda_corrupcao1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MOSTRA TÓPICOS DO LDA\n",
    "\n",
    "ldamodel.show_topics(num_topics=10, num_words=20, log=False, formatted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPRIME O RESULTADO DO LDA COM NUM_TOPICS DE TÓPICOS\n",
    "\n",
    "print(ldamodel.print_topics(num_topics=10, num_words=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldatopics = ldamodel.show_topics(formatted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOTA O MODELO DO LDA\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "pyLDAvis.gensim.prepare(ldamodel, doc_matrix_termos, dicionario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEFINE NÚMERO ÓTIMO DE TÓPICOS DO LDA\n",
    "\n",
    "def evaluate_graph(dictionary, corpus, texts, limit):\n",
    "    \"\"\"\n",
    "    Function to display num_topics - LDA graph using c_v coherence\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    limit : topic limit\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    lm_list : List of LDA topic models\n",
    "    c_v : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    c_v = []\n",
    "    lm_list = []\n",
    "    for num_topics in range(1, limit):\n",
    "        lm = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
    "        lm_list.append(lm)\n",
    "        cm = CoherenceModel(model=lm, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        c_v.append(cm.get_coherence())\n",
    "        \n",
    "    # Show graph\n",
    "    x = range(1, limit)\n",
    "    plt.plot(x, c_v)\n",
    "    plt.xlabel(\"num_topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"c_v\"), loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "    return lm_list, c_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RODA O ALGORITMO ACIMA\n",
    "\n",
    "lmlist, c_v = evaluate_graph(dictionary=dicionario, corpus=doc_matrix_termos, texts=texto, limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmtopics = lmlist[5].show_topics(formatted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOTA A AVALIAÇÃO DE TÓPICOS DO LDA\n",
    "\n",
    "pyLDAvis.gensim.prepare(lmlist[2], doc_matrix_termos, dicionario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ENCONTRA OS MELHORES TÓPICOS DO LDA\n",
    "\n",
    "def ret_top_model():\n",
    "    \"\"\"\n",
    "    Since LDAmodel is a probabilistic model, it comes up different topics each time we run it. To control the\n",
    "    quality of the topic model we produce, we can see what the interpretability of the best topic is and keep\n",
    "    evaluating the topic model until this threshold is crossed. \n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    lm: Final evaluated topic model\n",
    "    top_topics: ranked topics in decreasing order. List of tuples\n",
    "    \"\"\"\n",
    "    top_topics = [(0, 0)]\n",
    "    while top_topics[0][1] < 0.97:\n",
    "        lm = LdaModel(corpus=doc_matrix_termos, id2word=dicionario)\n",
    "        coherence_values = {}\n",
    "        for n, topic in lm.show_topics(num_topics=-1, formatted=False):\n",
    "            topic = [word for word, _ in topic]\n",
    "            cm = CoherenceModel(topics=[topic], texts=texto, dictionary=dicionario, window_size=10)\n",
    "            coherence_values[n] = cm.get_coherence()\n",
    "        top_topics = sorted(coherence_values.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return lm, top_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RODA O ALGORITMO ACIMA\n",
    "\n",
    "lm, top_topics = ret_top_model()\n",
    "print(top_topics[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AVALIA TODOS OS MÉTODOS ESTATÍSTICOS \n",
    "\n",
    "lsitopics = [[word for word, prob in topic] for topicid, topic in lsitopics]\n",
    "\n",
    "hdptopics = [[word for word, prob in topic] for topicid, topic in hdptopics]\n",
    "\n",
    "ldatopics = [[word for word, prob in topic] for topicid, topic in ldatopics]\n",
    "\n",
    "lmtopics = [[word for word, prob in topic] for topicid, topic in lmtopics]\n",
    "\n",
    "lsi_coherence = CoherenceModel(topics=lsitopics[:10], texts=texto, dictionary=dicionario, window_size=10).get_coherence()\n",
    "\n",
    "hdp_coherence = CoherenceModel(topics=hdptopics[:10], texts=texto, dictionary=dicionario, window_size=10).get_coherence()\n",
    "\n",
    "lda_coherence = CoherenceModel(topics=ldatopics, texts=texto, dictionary=dicionario, window_size=10).get_coherence()\n",
    "\n",
    "lm_coherence = CoherenceModel(topics=lmtopics, texts=texto, dictionary=dicionario, window_size=10).get_coherence()\n",
    "\n",
    "#lda_lsi_coherence = CoherenceModel(topics=lda_lsi_topics[:10], texts=texto, dictionary=dicionario, window_size=10).get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOTA O GRÁFICO DE AVALIAÇÃO\n",
    "\n",
    "def evaluate_bar_graph(coherences, indices):\n",
    "    \"\"\"\n",
    "    Function to plot bar graph.\n",
    "    \n",
    "    coherences: list of coherence values\n",
    "    indices: Indices to be used to mark bars. Length of this and coherences should be equal.\n",
    "    \"\"\"\n",
    "    assert len(coherences) == len(indices)\n",
    "    n = len(coherences)\n",
    "    x = np.arange(n)\n",
    "    plt.bar(x, coherences, width=0.2, tick_label=indices, align='center')\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Coherence Value')\n",
    "    \n",
    "    \n",
    "#evaluate_bar_graph([lsi_coherence, hdp_coherence, lda_coherence, lm_coherence, lda_lsi_coherence],\n",
    "#                   ['LSI', 'HDP', 'LDA', 'LDA_Mod', 'LDA_LSI'])\n",
    "\n",
    "evaluate_bar_graph([lsi_coherence, hdp_coherence, lda_coherence, lm_coherence],\n",
    "                   ['LSI', 'HDP', 'LDA', 'LDA_Mod'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
